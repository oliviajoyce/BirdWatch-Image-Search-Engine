{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05893c8a",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07005604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now import the required NLTK modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from datetime import timedelta\n",
    "import gunicorn\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ea081",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16063c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenization and Lowercasing\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation, stopwords, and perform stemming\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove punctuation and check if token is not empty after stripping\n",
    "        token = token.strip(string.punctuation)\n",
    "        if token != '' and len(token) >= 2:\n",
    "            # Perform stemming and filter out stopwords\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token not in stop_words:\n",
    "                processed_tokens.append(stemmed_token)\n",
    "    return processed_tokens  # return as list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbeb1b",
   "metadata": {},
   "source": [
    "# Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f39eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_path(file_name):\n",
    "    # Assuming notebook is in the same directory as the static folder\n",
    "    notebook_dir = os.getcwd()\n",
    "    static_folder = os.path.join(notebook_dir, 'static')\n",
    "    return os.path.join(static_folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "268d963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inverted_index(file_path):\n",
    "    inverted_index = {}\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line into term and postings\n",
    "            term, postings_str = line.strip().split(\":\", 1)\n",
    "            # Convert postings string to list of dictionaries\n",
    "            postings = eval(postings_str)\n",
    "            # Create a dictionary entry for the term\n",
    "            inverted_index[term] = postings\n",
    "    return inverted_index\n",
    "\n",
    "# Assuming your inverted index file is located in a 'static' folder in the same directory as your script\n",
    "# Construct the file path dynamically using the get_static_path function\n",
    "inverted_index_path = get_static_path('updated_inverted_index.txt')\n",
    "inverted_index = load_inverted_index(inverted_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635bdcd",
   "metadata": {},
   "source": [
    "# Ranking: BM-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c9001cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term, N, doc_freq):\n",
    "    return math.log((N - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "\n",
    "def compute_bm25(inverted_index, query_terms, k1=1.5, b=0.75):\n",
    "    bm25_scores = []\n",
    "    N = len(inverted_index)  # Total number of images\n",
    "    total_text_length = sum(sum(posting['term_frequency'] for posting in postings) for postings in inverted_index.values())\n",
    "    avgdl = total_text_length / N  # Average document length\n",
    "    \n",
    "    for term in query_terms:\n",
    "        idf_val = idf(term, N, len(inverted_index.get(term, [])))\n",
    "        if idf_val == 0:\n",
    "            continue  # Skip terms with IDF of 0\n",
    "        for posting in inverted_index.get(term, []):\n",
    "            doc_id = posting['image']\n",
    "            doc_len = sum(posting['term_frequency'] for posting in inverted_index[term])  # Assuming all terms contribute to the document length\n",
    "            # Calculate BM25 term score\n",
    "            term_score = idf_val * (posting['term_frequency'] * (k1 + 1)) / (posting['term_frequency'] + k1 * (1 - b + b * (doc_len / avgdl)))\n",
    "            bm25_scores.append((term, doc_id, term_score))\n",
    "    \n",
    "    return bm25_scores\n",
    "\n",
    "def rank_bm25(query, inverted_index, k1=1.5, b=0.75):\n",
    "    query_terms = preprocess(query)  # Assuming the query is already preprocessed\n",
    "    bm25_scores = compute_bm25(inverted_index, query_terms, k1, b)\n",
    "    # Filter images that don't contain all query terms\n",
    "    relevant_docs = set(posting[1] for posting in bm25_scores)\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            relevant_docs.intersection_update(posting['image'] for posting in inverted_index[term])\n",
    "    ranked_docs = [posting for posting in bm25_scores if posting[1] in relevant_docs]\n",
    "    return ranked_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045e2a4",
   "metadata": {},
   "source": [
    "# Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12ce2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google Cloud Storage client\n",
    "notebook_dir = os.getcwd()\n",
    "key_file_path = os.path.join(notebook_dir, \"poised-vial-419810-e53ec981e979.json\")\n",
    "credentials = service_account.Credentials.from_service_account_file(key_file_path)\n",
    "storage_client = storage.Client(credentials=credentials)\n",
    "bucket_name = \"bird_images_bucket\"\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0223b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_url(image_id, bucket):\n",
    "    # Assuming the bucket variable is a Google Cloud Storage Bucket object\n",
    "    # Check for possible file extensions\n",
    "    possible_extensions = ['.jpg', '.JPG', '']\n",
    "    blob = None\n",
    "\n",
    "    # Try to find a blob that exists with the given extensions\n",
    "    for ext in possible_extensions:\n",
    "        blob_path = f\"images/{image_id}{ext}\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        if blob.exists():\n",
    "            break\n",
    "        blob = None  # Reset if not found\n",
    "\n",
    "    # If no valid blob is found, return None or raise an exception\n",
    "    if not blob:\n",
    "        return None  # or raise Exception(\"Image not found.\")\n",
    "\n",
    "    # Generate a signed URL for the found blob\n",
    "    url = blob.generate_signed_url(expiration=timedelta(minutes=30))  # URL expires in 30 minutes\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3edcef3",
   "metadata": {},
   "source": [
    "# Remove Duplicates - OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "411a5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Compare images using OpenCV feature extraction\n",
    "def compute_image_features(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            img = cv2.imdecode(np.frombuffer(response.content, np.uint8), -1)\n",
    "            if img is not None:\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                # Use a feature extraction technique like ORB\n",
    "                orb = cv2.ORB_create()\n",
    "                keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "                return descriptors\n",
    "            else:\n",
    "                print(f\"Error: Failed to decode image from URL {image_url}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error: Unable to fetch image from URL {image_url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Group similar images together\n",
    "def find_similar_images(inverted_index):\n",
    "    similar_images = defaultdict(list)\n",
    "    for keyword, image_info_list in inverted_index.items():\n",
    "        for image_info in image_info_list:\n",
    "            image_path = image_info['positions']\n",
    "            features = compute_image_features(image_path)\n",
    "            if features is not None:\n",
    "                similar_images[keyword].append({'image': image_info['image'], 'features': features})\n",
    "    return similar_images\n",
    "\n",
    "#Identify representative images from each group\n",
    "def identify_representative_images(similar_images):\n",
    "    representative_images = {}\n",
    "    for keyword, images_info in similar_images.items():\n",
    "        representative_images[keyword] = []\n",
    "        # Choose a representative image based on a criteria, e.g., most features\n",
    "        representative_image = max(images_info, key=lambda x: len(x['features']))\n",
    "        representative_images[keyword].append(representative_image)\n",
    "    return representative_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502739ce",
   "metadata": {},
   "source": [
    "# Retrieval: Return Images by User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e3c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images_by_query(query_tokens, selected_country, inverted_index, file_path, run_name):  \n",
    "    # Define the path to the images metadata file\n",
    "    metadata_file = get_static_path(\"textual_surrogate2.txt\")\n",
    "    # Initialize a dictionary to store image metadata\n",
    "    image_metadata = {}\n",
    "    # Read the metadata file and parse its contents as JSON\n",
    "    with open(metadata_file, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        # Calculate BM25 scores for images\n",
    "        bm25_scores = rank_bm25(' '.join(query_tokens), inverted_index)\n",
    "        write_results(bm25_scores, file_path, run_name)\n",
    "        # Get the image metadata based on the ranked image IDs\n",
    "        for term, image_id, score in bm25_scores:\n",
    "            image_entry = next((entry for entry in data if entry['id'] == image_id), None)\n",
    "            if image_entry:\n",
    "                image_metadata[image_id] = image_entry\n",
    "                image_metadata[image_id]['image_url'] = get_image_url(image_id, bucket)  # Update image URL\n",
    "                image_metadata[image_id]['original_caption'] = image_entry['original_caption']\n",
    "                image_metadata[image_id]['original_country'] = image_entry['original_country']\n",
    "              \n",
    "                \n",
    "    # Step 6: Filter out duplicate images based on image features\n",
    "    def filter_duplicates(image_metadata):\n",
    "        unique_image_metadata = []\n",
    "        seen_features = set()\n",
    "        for image in image_metadata.values():\n",
    "            features = compute_image_features(image['image_url'])  #image_path\n",
    "            if features is not None:\n",
    "                features_tuple = tuple(tuple(row) for row in features)  # Convert NumPy array to tuple of tuples\n",
    "                hash_value = hash(features_tuple)\n",
    "                if hash_value not in seen_features:\n",
    "                    unique_image_metadata.append(image)\n",
    "                    seen_features.add(hash_value)\n",
    "        return unique_image_metadata\n",
    "\n",
    "    # Filter out duplicate images\n",
    "    unique_image_metadata = filter_duplicates(image_metadata)\n",
    "\n",
    "    return unique_image_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25ea00",
   "metadata": {},
   "source": [
    "# Output Model Results per Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a6105dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(results, file_path, run_name):\n",
    "    print(\"Results:\", results)  # Add this line to inspect the results\n",
    "    with open(file_path, 'w') as f:\n",
    "        rank = 1  # Initialize rank counter\n",
    "        for result in results:\n",
    "            if result[2] != 0.0:  # Exclude results with score 0.0\n",
    "                query_id =  str(result[0])\n",
    "                document_id =  str(result[1])\n",
    "                score = result[2]\n",
    "                # Write in TREC format: <query_id> <Q0> <doc_id> <rank> <score> <run_id>\n",
    "                f.write(f\"{query_id} Q0 {document_id} {rank} {score} {run_name}\\n\")\n",
    "                rank += 1  # Increment rank for the next document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31432f",
   "metadata": {},
   "source": [
    "# Web Application - Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b160667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the textual_surrogate2.txt file to extract unique country values\n",
    "textual_surrogate_file= get_static_path(\"original_textual_surrogate.txt\")\n",
    "with open(textual_surrogate_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract unique country values\n",
    "unique_countries = sorted(set(entry['country'] for entry in data))\n",
    "\n",
    "# Modify the image entries to include original captions\n",
    "for entry in data:\n",
    "    entry['original_caption'] = entry['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57e53bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('combined.html', countries = unique_countries)\n",
    "\n",
    "@app.route('/search-results', methods=['POST'])\n",
    "def handle_form():\n",
    "    if request.method == 'POST':\n",
    "        # Get the search keywords and selected country from the form\n",
    "        search_keywords = request.form.get('message')\n",
    "        selected_country = request.form.get('country')\n",
    "\n",
    "        # Combine search keywords and selected country if both are provided\n",
    "        if search_keywords and selected_country:\n",
    "            search_query = f\"{search_keywords} {selected_country}\"\n",
    "        else:\n",
    "            # Use either search keywords or selected country if one of them is provided\n",
    "            search_query = search_keywords or selected_country\n",
    "\n",
    "        # Preprocess the combined search query\n",
    "        processed_query = preprocess(search_query)\n",
    "\n",
    "        # Filter images based on the processed query\n",
    "        file_path = get_static_path('results.txt')\n",
    "        image_metadata = filter_images_by_query(processed_query, selected_country, inverted_index, file_path, run_name=search_query)\n",
    "        num_images = len(image_metadata)\n",
    "\n",
    "        # Render the gallery page with the filtered images\n",
    "        return render_template('combined.html', country=search_query, num_images=num_images, images=image_metadata, countries=unique_countries, show_dynamic=True)\n",
    "    else:\n",
    "        # If the method is not POST, render the gallery page without the dynamic content\n",
    "        return render_template('combined.html', show_dynamic=False, countries=unique_countries)\n",
    "\n",
    "\n",
    "#app.run(debug=True, port=8080, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2773706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8080\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/bootstrap-icons/bootstrap-icons.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/swiper/swiper-bundle.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/glightbox/css/glightbox.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/css/main.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/aos/aos.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/bootstrap/js/bootstrap.bundle.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/aos/aos.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/js/main.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/glightbox/js/glightbox.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:22] \"GET /static/vendor/swiper/swiper-bundle.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:23] \"GET /static/vendor/bootstrap-icons/fonts/bootstrap-icons.woff2?dd67030699838ea613ee6dbda90effa6 HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [('ireland', '07f2a06c-060f-4db9-abaa-9bd12c5b2b88', 1.5265863814452612), ('robin', '07f2a06c-060f-4db9-abaa-9bd12c5b2b88', 3.987016329034715)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/May/2024 01:03:40] \"POST /search-results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/swiper/swiper-bundle.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/bootstrap-icons/bootstrap-icons.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/glightbox/css/glightbox.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/aos/aos.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/css/main.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/bootstrap/js/bootstrap.bundle.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/swiper/swiper-bundle.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/glightbox/js/glightbox.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/vendor/aos/aos.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2024 01:03:40] \"GET /static/js/main.js HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "app.run(port=8080)  # Change 5000 to another port number like 8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e1a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe89258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564e851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
